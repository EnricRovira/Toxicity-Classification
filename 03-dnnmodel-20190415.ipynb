{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import operator \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import re, unicodedata\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Dense, Conv1D, MaxPooling1D, Flatten, Dropout, SpatialDropout1D, LSTM, GlobalMaxPool1D, \\\n",
    "                        GlobalAveragePooling1D, concatenate, Bidirectional, CuDNNLSTM, CuDNNGRU, Masking, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Model, load_model\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.set_option('max_colwidth', 300)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crawl-300d-2M.vec']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('../input/fasttext-crawl-300d-2m/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\n",
    "path_glove = '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "path_fasttext = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "path_output = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path_data + 'train.csv')\n",
    "test = pd.read_csv(path_data + 'test.csv')\n",
    "sub = pd.read_csv(path_data + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train[['id','comment_text']], test], axis=0)\n",
    "del(train, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "                       \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "                       \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                       \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \n",
    "                       \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                       \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                       \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "                       \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \n",
    "                       \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                       \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "swear_words_re = ' 4r5e | 5h1t | 5hit | ass-fucker | assfucker | assfukka | asswhole | a_s_s | b!tch | b17ch | blow job \\\n",
    "                    | boiolas | bollok | boooobs | booooobs | booooooobs | bunny fucker | buttmuch | c0cksucker \\\n",
    "                    | carpet muncher | cl1t | cockface | cockmunch | cockmuncher | cocksuka | cocksukka | cokmuncher \\\n",
    "                    | coksucka | cunillingus | cuntlick | cuntlicker | cuntlicking | cyalis | cyberfuc | cyberfuck \\\n",
    "                    | cyberfucked | cyberfucker | cyberfuckers | cyberfucking | dirsa | dlck | dog-fucker | donkeyribber \\\n",
    "                    | ejaculatings | ejakulate | f u c k | f u c k e r | f4nny | faggitt | faggs | fannyflaps | fannyfucker \\\n",
    "                    | fanyy | fingerfucker | fingerfuckers | fingerfucks | fistfuck | fistfucked | fistfucker | fistfuckers \\\n",
    "                    | fistfucking | fistfuckings | fistfucks | fuckingshitmotherfucker | fuckwhit | fudge packer | fudgepacker \\\n",
    "                    | fukwhit | fukwit | fux0r | f_u_c_k | god-dam | kawk | knobead | knobed | knobend | knobjocky | knobjokey \\\n",
    "                    | kondum | kondums | kummer | kumming | kums | kunilingus | l3itch | m0f0 | m0fo | m45terbate | ma5terb8 \\\n",
    "                    | ma5terbate | master-bate | masterb8 | masterbat3 | masterbations | mof0 | mothafuck | mothafuckaz \\\n",
    "                    | mothafucked | mothafucking | mothafuckings | mothafucks | mother fucker | motherfucked | motherfuckings \\\n",
    "                    | motherfuckka | motherfucks | muthafecker | muthafuckker | n1gga | n1gger | nigg3r | nigg4h | nob jokey \\\n",
    "                    | nobjocky | nobjokey | penisfucker | phuked | phuking | phukked | phukking | phuks | phuq | pigfucker \\\n",
    "                    | pimpis | pissflaps | rimjaw | s hit | scroat | sh!t | shitdick | shitfull | shitings | shittings | s_h_i_t \\\n",
    "                    | t1tt1e5 | t1tties | teez | tittie5 | tittiefucker | tittywank | tw4t | twathead | twunter | v14gra \\\n",
    "                    | v1gra | w00se | whoar '\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n",
    "                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does',\n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
    "                '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', \n",
    "                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mapping = {\"_\":\" \", \"`\":\" \"}\n",
    "\n",
    "def clean_special_chars(text, puncts, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    for p in puncts:\n",
    "        text = text.replace(p, f' {p} ')     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(words):\n",
    "    for word in mispell_dict.keys():\n",
    "        words = words.replace(word, mispell_dict[word])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    return re.sub('\\d+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strange_words(x):\n",
    "     return re.sub('\\W*\\b\\w{20,100}\\b', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_swears(text):\n",
    "    text = re.sub(swear_words_re, ' fuck ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 38s, sys: 544 ms, total: 3min 38s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: x.lower())\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df['comment_text'] = df['comment_text'].apply(lambda x: handle_swears(x))\n",
    "df['comment_text'] = df['comment_text'].apply(correct_spelling)\n",
    "df['comment_text'] = df['comment_text'].apply(clean_numbers)\n",
    "df['comment_text'] = df['comment_text'].apply(remove_strange_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>this is so cool .  it is like ,    '  would you want your mother to read this ?  ?   '   really great idea ,  well done !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>thank you !  !  this would make my life a lot less anxiety  -  inducing .  keep it up ,  and do not let anyone get in your way !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>this is such an urgent design problem ;  kudos to you for taking it on .  very impressive !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>is this something i will be able to install on my site ?  when will you be releasing it ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                                                                                                       comment_text\n",
       "0  59848         this is so cool .  it is like ,    '  would you want your mother to read this ?  ?   '   really great idea ,  well done ! \n",
       "1  59849  thank you !  !  this would make my life a lot less anxiety  -  inducing .  keep it up ,  and do not let anyone get in your way ! \n",
       "2  59852                                       this is such an urgent design problem ;  kudos to you for taking it on .  very impressive ! \n",
       "3  59855                                         is this something i will be able to install on my site ?  when will you be releasing it ? \n",
       "4  59856                                                                                             haha you guys are a bunch of losers . "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.iloc[:1804874,:]\n",
    "test = df.iloc[1804874:,:]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['target',\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "\n",
    "train_orig = pd.read_csv( path_data + \"train.csv\")\n",
    "train = pd.concat([train, train_orig[my_columns]], axis=1)\n",
    "del(train_orig)\n",
    "# gc.collect()\n",
    "train['target'] = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1624386 train comments, 180488 validate comments\n"
     ]
    }
   ],
   "source": [
    "train_df, validate_df = train_test_split(train, test_size=0.1, random_state = 12)\n",
    "print(f'{len(train_df)} train comments, {len(validate_df)} validate comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 301731 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 100_000 #574_312 unique tokens in word_index\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 220\n",
    "\n",
    "corpus = pd.concat([train_df['comment_text'], validate_df ['comment_text'], test['comment_text']])\n",
    "corpus = corpus.drop_duplicates()\n",
    "\n",
    "# Create a text tokenizer.\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = pad_sequences(tokenizer.texts_to_sequences(train_df['comment_text']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "train_labels = train_df['target']\n",
    "validate_text = pad_sequences(tokenizer.texts_to_sequences(validate_df['comment_text']), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "validate_labels = validate_df['target']\n",
    "test_text = pad_sequences(tokenizer.texts_to_sequences(test['comment_text']), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del (df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 36s, sys: 5.52 s, total: 2min 41s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_matrix = build_matrix(tokenizer.word_index, path_glove)\n",
    "#fasttext_matrix = build_matrix(tokenizer.word_index, path_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "y_aux_validate = validate_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "num_categories_train = y_aux_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix):\n",
    "    sequence_input = Input(shape = (MAX_SEQUENCE_LENGTH,),  dtype='int32')\n",
    "    \n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(sequence_input)\n",
    "\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "\n",
    "    x1 = Bidirectional(CuDNNLSTM(128, return_sequences = True))(x)\n",
    "    x2 = Bidirectional(CuDNNLSTM(64, return_sequences = True))(x1)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x2)\n",
    "    max_pool = GlobalMaxPooling1D()(x2)\n",
    "\n",
    "    vf = concatenate([avg_pool, max_pool])\n",
    "    vf = BatchNormalization()(vf)\n",
    "\n",
    "    #vf = Dropout(0.2)(Dense(64, activation='relu') (vf))\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid', name = 'target')(vf)\n",
    "    preds_aux = Dense(num_categories_train, activation = 'sigmoid', name = 'categories')(vf)\n",
    "\n",
    "    model = Model(inputs = sequence_input, outputs=[preds, preds_aux])\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                      optimizer = 'adam',\n",
    "                      metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(embedding_matrix):\n",
    "    NUM_MODELS = 2\n",
    "    EPOCHS = 4\n",
    "    checkpoint_predictions_val = []\n",
    "    checkpoint_predictions_test = []\n",
    "    weights_val = []\n",
    "    weights_test = []\n",
    "    file_path = \"best_model.hdf5\"\n",
    "\n",
    "    for model_i in range(NUM_MODELS):\n",
    "        model = build_model(embedding_matrix)\n",
    "        for glob_epoch in range (EPOCHS):\n",
    "            model.fit(train_text,\n",
    "               [train_labels, y_aux_train],\n",
    "               batch_size=512,\n",
    "               #class_weight = class_weight,\n",
    "               epochs=1,\n",
    "               validation_data=(validate_text, [validate_labels, y_aux_validate]),\n",
    "               callbacks = [EarlyStopping(monitor='val_categories_loss', mode='min', verbose=1, patience=1),             \n",
    "                            ModelCheckpoint(file_path, monitor = \"val_categories_loss\", verbose = 1,\n",
    "                                            save_best_only = True, mode = \"min\"),\n",
    "                           LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** glob_epoch))],\n",
    "               verbose=0) #Avoid kaggle kernels issues\n",
    "            \n",
    "            checkpoint_predictions_val.append(model.predict(validate_text, batch_size=2048)[0].flatten())\n",
    "            weights_val.append(2 ** glob_epoch)\n",
    "            checkpoint_predictions_test.append(model.predict(test_text, batch_size=2048)[0].flatten())\n",
    "            weights_test.append(2 ** glob_epoch)\n",
    "            \n",
    "    predictions_val = np.average(checkpoint_predictions_val, weights=weights_val, axis=0)\n",
    "    predictions_test = np.average(checkpoint_predictions_test, weights=weights_test, axis=0)\n",
    "            \n",
    "    return predictions_val, predictions_test\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08967, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08566, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08742, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08323, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.09321, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08398, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08708, saving model to best_model.hdf5\n",
      "\n",
      "Epoch 00001: val_categories_loss improved from inf to 0.08310, saving model to best_model.hdf5\n",
      "CPU times: user 28min 40s, sys: 28min 48s, total: 57min 28s\n",
      "Wall time: 1h 9min 51s\n"
     ]
    }
   ],
   "source": [
    "%time glove_val_preds, glove_test_preds = train_model(glove_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df['predict_NN'] = glove_val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[163597   2414]\n",
      " [  5908   8569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    166011\n",
      "           1       0.78      0.59      0.67     14477\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    180488\n",
      "   macro avg       0.87      0.79      0.82    180488\n",
      "weighted avg       0.95      0.95      0.95    180488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(validate_df.target, np.where(validate_df.predict_NN > 0.5, 1, 0)))\n",
    "print(classification_report(validate_df.target, np.where(validate_df.predict_NN > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.9658466871475452\n"
     ]
    }
   ],
   "source": [
    "print('ROC-AUC: {}'.format(roc_auc_score(validate_df.target, validate_df.predict_NN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>jeff sessions is another one of trump  '  s orwellian choices .  he believes and has believed his entire career the exact opposite of what the position requires .</td>\n",
       "      <td>0.004802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>i actually inspected the infrastructure on grand chief stewart philip  '  s home penticton first nation in both   and   .   exactly zero projects that had been identified in previous inspection reports had been funded by the federal government ,  and the entire band was housed in atco trailers ....</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>no it will not  .  that is just wishful thinking on democrats fault  .    for the   th time  ,  walker cited the cost of drug users treatment as being lost with obamacare  .   i laugh every time i hear a liberal claim republicans want to hurt people  ,  and that is why they dumped obamacare .</td>\n",
       "      <td>0.005147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>instead of wringing our hands and nibbling the periphery of the issue ,  how about we face the actual issue head on ?  i would support a city ordinance against loitering ,  and applaud city councilors who champion a real and permanent solution . \\n\\nthe details could be determined ,  but would i...</td>\n",
       "      <td>0.002219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>how many of you commenters have garbage piled high in your yard ,  bald tires ,  dead batteries ,  rotten pallets ,  car parts ,  blah blah blah .  this town is a pigpen .  drive around and look for yourself ,  its pathetic .</td>\n",
       "      <td>0.954110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    ...     prediction\n",
       "0  7000000    ...       0.004802\n",
       "1  7000001    ...       0.000103\n",
       "2  7000002    ...       0.005147\n",
       "3  7000003    ...       0.002219\n",
       "4  7000004    ...       0.954110\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['prediction'] = glove_test_preds\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
